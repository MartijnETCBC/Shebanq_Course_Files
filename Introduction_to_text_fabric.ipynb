{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know more about the features of the ETCBC-database, check the feature documentation on the Shebanq website:\n",
    "https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/0_overview.html\n",
    "\n",
    "Look also at the text-fabric API: https://github.com/ETCBC/text-fabric/wiki/Api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, collections, os\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Text-fabric wakes up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.2\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "108 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "from tf.fabric import Fabric\n",
    "TF = Fabric(modules='hebrew/etcbc4c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load the features that you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.26s B language             from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s Feature overview: 102 nodes; 5 edges; 1 configs; 7 computeds\n",
      "  0.34s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "api = TF.load('''\n",
    "    lex sp gn nu vt vs typ function prs book language\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     0.00s M otext                from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.26s B language             from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@am              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@ar              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@bn              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@da              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@de              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@el              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@en              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@es              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@fa              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@fr              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@he              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@hi              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@id              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@ja              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@ko              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@la              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@nl              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@pa              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@pt              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@ru              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@sw              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@syc             from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@tr              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@ur              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@yo              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s B book@zh              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M voc_utf8             from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M root                 from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_prs                from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M rank_lex             from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M ls                   from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M pfm                  from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_vbs                from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M pdp                  from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_nme                from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_uvf_utf8           from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M mother               from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_pfm_utf8           from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M vbs                  from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M freq_lex             from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M ps                   from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M voc                  from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M prs_nu               from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M uvf                  from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M nametype             from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M functional_parent    from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M st                   from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M pargr                from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M prs_ps               from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M freq_occ             from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M gloss                from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M dist                 from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M nme                  from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M rank_occ             from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_prs_utf8           from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_pfm                from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M det                  from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M omap@4b-4c           from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M instruction          from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M is_root              from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M number               from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M rela                 from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M label                from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M domain               from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M code                 from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_nme_utf8           from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M mother_object_type   from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M kind                 from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M vbe                  from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M dist_unit            from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M distributional_parent from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M prs_gn               from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M tab                  from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M otext                from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_vbe                from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_vbs_utf8           from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_vbe_utf8           from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M g_uvf                from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n",
      "   |     0.00s M txt                  from C:/Users/Martijn/github/text-fabric-data/hebrew/etcbc4c\n"
     ]
    }
   ],
   "source": [
    "api.loadLog()\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects\n",
    "Which objects are there in the ETCBC database and how many of each? In the following cell they are counted. \n",
    "The total number of objects is called n and it is initialized as 0. The different types of objects are counted in the dictionary called 'object_types'. This is a defaultdict() from the collections module. Using the defaultdict instaed of an ordinary dictionary has the advantage that new keys in the dictionary do not need to be initialized explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446130 defaultdict(<class 'int'>, {'book': 39, 'lex': 9236, 'verse': 23213, 'chapter': 929, 'half_verse': 45180, 'sentence_atom': 64339, 'word': 426581, 'subphrase': 113792, 'sentence': 63570, 'clause': 88000, 'phrase': 253174, 'phrase_atom': 267515, 'clause_atom': 90562})\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "object_types = collections.defaultdict(int)\n",
    "\n",
    "for node in N():\n",
    "    n += 1\n",
    "    object_types[F.otype.v(node)] += 1\n",
    "\n",
    "print(n, object_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With \"for node in N():\" you walk through all the nodes (or objects) in the database. This script will add 1 to the variable n every time it sees a new node. The next line of code is a bit more complex. Of every node, it asks what kind of object it is using the feature \"otype\". Note that this feature does not need to be initialized in cell 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When object_types is initialized it is still an empty dictionary. Once it encounters the first node, it checks the object-type, adds that object-type to the dictionary and adds 1 to its initial value 0. In F.otype.v(), F is the class of object features, which is followed by the name of the feature. This is followed by v, which stands for the value of the feature. The values for the feature otype are word, clause, sentence, and so on.\n",
    "\n",
    "If you want to check the object-type of the first node, use this script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.otype.v(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare print() with pprint() from the pprint module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>,\n",
      "            {'book': 39,\n",
      "             'chapter': 929,\n",
      "             'clause': 88000,\n",
      "             'clause_atom': 90562,\n",
      "             'half_verse': 45180,\n",
      "             'lex': 9236,\n",
      "             'phrase': 253174,\n",
      "             'phrase_atom': 267515,\n",
      "             'sentence': 63570,\n",
      "             'sentence_atom': 64339,\n",
      "             'subphrase': 113792,\n",
      "             'verse': 23213,\n",
      "             'word': 426581})\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(object_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The result is clear, the database contains 39 books, 929 chapters, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we would not have used the defaultdict(), the script would have looked like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1446130 defaultdict(<class 'int'>, {'sentence': 63570, 'clause': 88000, 'subphrase': 113792, 'word': 426581, 'clause_atom': 90562, 'half_verse': 45180, 'lex': 9236, 'phrase_atom': 267515, 'verse': 23213, 'phrase': 253174, 'chapter': 929, 'book': 39, 'sentence_atom': 64339})\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "object_types = collections.defaultdict(int)\n",
    "\n",
    "for node in N():\n",
    "    n += 1\n",
    "    if F.otype.v(node) in object_types:\n",
    "        object_types[F.otype.v(node)] += 1\n",
    "    else:\n",
    "        object_types[F.otype.v(node)] = 1\n",
    "        #the object-type has to be initialized\n",
    "\n",
    "print(n, object_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a more efficient way of walking through the nodes. In general you do not need information from all the objects, but only from one specific object-type, for instance words. If this is the case, you do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426581\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "\n",
    "for word in F.otype.s('word'):\n",
    "    word_count += 1\n",
    "    \n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "If you want to know which range of slots is used for one specific object, you use sInterval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 426581)\n",
      "(426582, 514581)\n"
     ]
    }
   ],
   "source": [
    "print(F.otype.sInterval('word'))\n",
    "print(F.otype.sInterval('clause'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects that we have encountered are abstract entities, and if you want to know what concrete properties an object has, you need to use the features that characterize a certain object. As we have seen, the slots 1 upt to 426581 represent words. If we want to know what the lexemes are of the first 10 word slots, we use the feature \"lex\", which was initialized in cell 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "R>CJT/\n",
      "BR>[\n",
      ">LHJM/\n",
      ">T\n",
      "H\n",
      "CMJM/\n",
      "W\n",
      ">T\n",
      "H\n"
     ]
    }
   ],
   "source": [
    "for word_slot in range(1, 11):\n",
    "    print(F.lex.v(word_slot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You recognize that these are the lexemes of the first clause in the book of Genesis. However, in which book can you find word slot 100000? To be able to locate it, use T.sectionfromNode(), which returns a tuple with the book (by default in English), chapter and verse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Deuteronomy', 11, 19)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.sectionFromNode(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need the name of the book in a different language, youse the argument \"lang\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Deutéronome', 11, 19)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.sectionFromNode(100000, lang = 'fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen the feature \"lex\", which is a word feature. Other important word features are \"sp\" (part of speech), \"gn\" (gender), \"nu\" (number), \"vt\" (verbal tense), \"vs\" (verbal stem). The latter two have a value only if the word is a verb. Suppose you want to know the values of all of these features of the forst 10 words of Genesis, we do this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B prep NA NA NA NA\n",
      "R>CJT/ subs f sg NA NA\n",
      "BR>[ verb m sg perf qal\n",
      ">LHJM/ subs m pl NA NA\n",
      ">T prep NA NA NA NA\n",
      "H art NA NA NA NA\n",
      "CMJM/ subs m pl NA NA\n",
      "W conj NA NA NA NA\n",
      ">T prep NA NA NA NA\n",
      "H art NA NA NA NA\n"
     ]
    }
   ],
   "source": [
    "for word in F.otype.s('word'):\n",
    "    if word < 11: #an alternative way of finding the first 10 words\n",
    "        print(F.lex.v(word), F.sp.v(word), F.gn.v(word), F.nu.v(word), F.vt.v(word), F.vs.v(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a feature has no value in the case of a specific word, NA is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need features that are characteristic of phrases or clauses. Suppose we want to know the types and functions of the first 10 phrases in the first book of Genesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(605144, 858317)\n"
     ]
    }
   ],
   "source": [
    "print(F.otype.sInterval('phrase'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PP Time\n",
      "VP Pred\n",
      "NP Subj\n",
      "PP Objc\n",
      "CP Conj\n",
      "NP Subj\n",
      "VP Pred\n",
      "NP PreC\n",
      "CP Conj\n",
      "NP Subj\n"
     ]
    }
   ],
   "source": [
    "for phrase_slot in range(605144, 605154):\n",
    "    print(F.typ.v(phrase_slot), F.function.v(phrase_slot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very often you may not only interested be interested in the features of specific objects, but also of other objects in its environment. Suppose you are interested in eating habits in the Hebrew Bible. You decide to search for cases of the verb >KL[ (to eat), used as the predicate of a clause, and you are interested in those cases in which that clause has an explicit subject. You would like to know all the lexemes in the subject.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy is as follows. Fist we search for all cases of >KL[. From the word >KL[ we move upward to the phrase in which it occurs, using L.u(). Of this phrase it is checked if it is a predicate. We move upward again, to the level of the clause, and in the clause we check if there is an explicit subject, by moving downwards to the phrases in the clause. It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'N<R/']\n",
      "['XRB=/']\n",
      "['BN/', 'JFR>L/']\n",
      "['XJH/', 'R</']\n",
      "['XJH/', 'R</']\n",
      "['HW>']\n",
      "['H', '<WP/']\n",
      "['H', '<WP/']\n",
      "['H', 'PRH/', 'R</', 'H', 'MR>H/', 'W', 'DQ/', 'H', 'BFR/']\n",
      "['H', 'PRH/', 'H', 'RQ=/', 'W', 'H', 'R</']\n",
      "['H', '>JC/']\n",
      "['KL/', 'BN/', 'NKR/']\n",
      "['TWCB/', 'W', 'FKJR/']\n",
      "['KL/', '<RL/']\n",
      "['XMY/']\n",
      "['BN/', 'JFR>L/']\n",
      "['>T', 'BFR/']\n",
      "['GDJC/', '>W', 'H', 'QMH/', '>W', 'H', 'FDH/']\n",
      "['>BJWN/', '<M/']\n",
      "['XJH/', 'H', 'FDH/']\n",
      "['MR>H/', 'KBWD/', 'JHWH/']\n",
      "['>HRN/', 'W', 'BN/']\n",
      "['ZR/']\n",
      "['H', '>C/']\n",
      "['>HRN/', 'W', 'BN/']\n",
      "['KL/', 'ZKR=/', 'B', 'BN/', '>HRN/']\n",
      "['H', 'KHN/']\n",
      "['KL/', 'ZKR=/', 'B', 'H', 'KHN/']\n",
      "['KL/', 'XV>T/']\n",
      "['KL/', 'ZKR=/', 'B', 'H', 'KHN/']\n",
      "['BFR/', 'ZBX/', 'TWDH/', 'CLM/']\n",
      "['H', 'BFR/']\n",
      "['KL/', 'VHR/']\n",
      "['>HRN/', 'W', 'BN/']\n",
      "['>TH', 'W', 'BN/', 'W', 'BT/', '>T==']\n",
      "['KL/', 'NPC/', 'MN']\n",
      "['H', 'GR/']\n",
      "['KL/', 'ZR/']\n",
      "['TWCB/', 'KHN/', 'W', 'FKJR/']\n",
      "['HW>']\n",
      "['HM']\n",
      "['HJ>']\n",
      "['BT/', 'KHN/']\n",
      "['KL/', 'ZR/']\n",
      "['>JB[']\n",
      "['>RY/', '>JB[']\n",
      "['MJ']\n",
      "['MJ']\n",
      "['XYJ/', 'BFR/']\n",
      "['KL/', 'ZKR=/']\n",
      "['KL/', 'VHR/', 'B', 'BJT/']\n",
      "['KL/', 'VHR/', 'B', 'BJT/']\n",
      "['>TM', 'W', 'BJT/']\n",
      "['H', '<M/']\n",
      "['H', '>C/']\n",
      "['HW>']\n",
      "['H', '>C/', 'H', 'GDWL/', 'H', 'Z>T']\n",
      "['HW>']\n",
      "['H', 'VM>/', 'W', 'H', 'VHR/']\n",
      "['>TH', 'W', 'BN/', 'W', 'BT/', 'W', '<BD/', 'W', '>MH/', 'W', 'H', 'LWJ/']\n",
      "['>T', 'H', 'YBJ=/', 'W', '>T', 'H', '>JL/']\n",
      "['H', 'VM>/', 'W', 'H', 'VHR/']\n",
      "['>TH', 'W', 'BJT/']\n",
      "['<M/']\n",
      "['H', 'TWL<T/']\n",
      "['XRB/']\n",
      "['>TM']\n",
      "['CNJM/']\n",
      "['CNJM/']\n",
      "['H', '<M/']\n",
      "['H', 'QR>[']\n",
      "['C>WL=/']\n",
      "['H', '<M/']\n",
      "['H', '<M/']\n",
      "['XRB/']\n",
      "['>TH']\n",
      "['MPJBCT/', 'BN/', '>DWN/']\n",
      "['MPJBCT/']\n",
      "['HW>']\n",
      "['H', 'XRB/']\n",
      "['H', 'N<R/']\n",
      "['H', 'XRB/']\n",
      "['>C/', 'MN', 'PH/']\n",
      "['H', '>RJH/']\n",
      "['H', 'KLB/']\n",
      "['<WP/', 'H', 'CMJM/']\n",
      "['H', 'KLB/']\n",
      "['<WP/', 'H', 'CMJM/']\n",
      "['HJ>', 'W', 'HW>', 'W', 'BJT/']\n",
      "['H', 'KLB/']\n",
      "['H', 'KLB/']\n",
      "['<WP/', 'H', 'CMJM/']\n",
      "['H', 'KLB/']\n",
      "['H', 'KLB/']\n",
      "['ZR/']\n",
      "['GWR[']\n",
      "['LCWN/', '>C/']\n",
      "['KL/']\n",
      "['>RJH/']\n",
      "['>LH=/']\n",
      "['>P', '>C/', 'YR=/']\n",
      "['H', '>LP/', 'W', 'H', '<JR=/']\n",
      "['LCWN/']\n",
      "['XRB/']\n",
      "['<C/']\n",
      "['<C/']\n",
      "['SS/']\n",
      "['<BD/']\n",
      "['>XR=/']\n",
      "['>RJH/']\n",
      "['XRB/']\n",
      "['H', 'BCT/']\n",
      "['BN/', 'W', 'BT/']\n",
      "['XRB/', 'L', 'JHWH/']\n",
      "['KL/', '>KL[']\n",
      "['>B/']\n",
      "['H', 'BSR/']\n",
      "['XRB/']\n",
      "['XRB/']\n",
      "['KL/', 'MY>[']\n",
      "['MLK/', '>CWR/']\n",
      "['BN/', 'JFR>L/']\n",
      "['>B/']\n",
      "['BN/']\n",
      "['R<B/', 'W', 'DBR=/']\n",
      "['H', '>C/']\n",
      "['>C/']\n",
      "['H', '>C/']\n",
      "['>B/']\n",
      "['>C/']\n",
      "['>XRJT/']\n",
      "['HMH']\n",
      "['HJ>']\n",
      "['XJH/', 'H', '>RY/']\n",
      "['>T=']\n",
      "['>TJQ/']\n",
      "['H', 'KHN/']\n",
      "['HMH']\n",
      "['H', 'KHN/']\n",
      "['XJH/', 'H', 'FDH/']\n",
      "['XDC=/']\n",
      "['ZR/']\n",
      "['H', '>RBH/']\n",
      "['H', 'JLQ/']\n",
      "['H', 'XSJL/']\n",
      "['>C/']\n",
      "['>C/']\n",
      "['>C/']\n",
      "['H', '>RBH/', 'H', 'JLQ/', 'W', 'H', 'XSJL/', 'W', 'H', 'GZM/', 'XJL/', 'H', 'GDWL/']\n",
      "['H', 'GZM/']\n",
      "['>TH']\n",
      "['XRB/']\n",
      "['>C/']\n",
      "['>C/']\n",
      "['KL/', 'H', '>RY/']\n",
      "['KL/', 'H', '>RY/']\n",
      "['HJ>']\n",
      "['>C/']\n",
      "['>KL[', '<M/']\n",
      "['>C/']\n",
      "['>C/']\n",
      "['<NW/']\n",
      "['>C/']\n",
      "['>KL[', '<M/']\n",
      "['QN>H/', 'BJT/']\n",
      "['>JC/']\n",
      "['>C/']\n",
      "['BN/', 'W', 'BT/']\n",
      "['BN/', 'W', 'BT/']\n",
      "['R<B=/']\n",
      "['TPL=/']\n",
      "['<C/']\n",
      "['>C/']\n",
      "['BKR/', 'MWT/']\n",
      "['>C/']\n",
      "['>C/']\n",
      "['>XR=/']\n",
      "['JTWM/']\n",
      "['BHMWT/']\n",
      "['YDJQ/']\n",
      "['>HB[']\n",
      "['NYR[', 'T>NH/']\n",
      "['BN/', 'NCR/']\n",
      "['B<Z/']\n",
      "['MJ']\n",
      "['>JC/', 'NKRJ/']\n",
      "['FR/']\n",
      "['FR/']\n",
      "['>CH/']\n",
      "['QRY/', 'DJ', 'JHWDJ/']\n",
      "['BN/', 'JFR>L/', 'W', 'KL/']\n",
      "['C<R/']\n",
      "['C<R/']\n",
      "['>NJ', 'W', '>X/']\n"
     ]
    }
   ],
   "source": [
    "for word in F.otype.s('word'):\n",
    "    if F.lex.v(word) == '>KL[':\n",
    "        phrase = L.u(word, 'phrase')[0] # L.u() returns a tuple. We want to know the slot of the phrase, so we add the index [0], which is the first value of the tuple.\n",
    "        #now we check if the phrase is a predicate, we chech also for cases in a nominal predicate (PreC), and predicates with an object suffix (PreO):\n",
    "        if F.function.v(phrase) in {'Pred','PreC','PreO'}:\n",
    "            #we move upwards to the clause\n",
    "            clause = L.u(phrase, 'clause')[0]\n",
    "            #and we go down again to all the phrases in that clause\n",
    "            phrases = L.d(clause, 'phrase')\n",
    "            #we loop over all the phrases to check if there is an explicit subject:\n",
    "            for phrase in phrases:\n",
    "                if F.function.v(phrase) == 'Subj':\n",
    "                    #we create an empty list, in which all lexemes are stored.\n",
    "                    lex_list = []\n",
    "                    #we move down to the lexemes of the subject:\n",
    "                    words = L.d(phrase, 'word')\n",
    "                    for word in words:\n",
    "                        lex_list.append(F.lex.v(word))\n",
    "                    print(lex_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It can be useful to save the data in a csv file to process the data further outside text-fabric. We want to do that with the >KL[ data from the previous cell. On every row in the csv file we store one clause containing >KL[, with the following information in columns: \n",
    "slot of >KL[, \n",
    "book, \n",
    "chapter, \n",
    "verse,\n",
    "verbal tense, \n",
    "verbal stem\n",
    "predicate type (Pred, PreC, PreO, PreS)\n",
    "lexemes of the subject, concatenated in a string, separated by underscores.\n",
    "The first 6 columns contain information about the verb, the predicate type contains information about the phrase in which >KL[ occurs, and the last column contains information about the subject of the clause. It may look like a lot of work, but you will notice that it is done straightforwardly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#in a dictionary (eat_dict) we save all the rows that will be saved in the csv file. \n",
    "#the keys of the dictionary will be the >KL slots, the values are lists with information about the words and phrases\n",
    "#we want to save the data in the csv file in canonical order, so we store the >KL[ slots in a list (eat_list), becuse there is no order in the keys of the dictionary.\n",
    "\n",
    "eat_dict = {}\n",
    "eat_list = []\n",
    "\n",
    "#this part is nearly identical to what you have already seen\n",
    "for w in F.otype.s('word'):\n",
    "    #select the words with the right lexeme and make sure the language is Hebrew.\n",
    "    if F.lex.v(w) == '>KL[' and F.language.v(w) == 'hbo':\n",
    "        phrase = L.u(w, 'phrase')[0] \n",
    "        #we include cases with a subjectsuffix\n",
    "        if F.function.v(phrase) == 'PreS':\n",
    "            suffix = F.prs.v(w)\n",
    "            #now we collect the information needed\n",
    "            where = T.sectionFromNode(w)\n",
    "            info = [w, where[0], where[1], where[2], F.vt.v(w), F.vs.v(w), F.function.v(phrase), suffix]\n",
    "            eat_dict[w] = info\n",
    "            eat_list.append(w)\n",
    "        #here the other predicate types are processed\n",
    "        else:\n",
    "            if F.function.v(phrase) in {'Pred','PreC','PreO'}:\n",
    "                where = T.sectionFromNode(w)\n",
    "                clause = L.u(phrase, 'clause')[0]\n",
    "                phrases = L.d(clause, 'phrase')\n",
    "                subject = False #we only include those cases that have an explicit subject\n",
    "                for phr in phrases:\n",
    "                    if F.function.v(phr) == 'Subj':\n",
    "                        subject = True\n",
    "                        lex_list = []\n",
    "                        words = L.d(phr, 'word')\n",
    "                        subj_lexemes = ''\n",
    "                        for word in words:\n",
    "                            if not word == words[-1]:\n",
    "                                subj_lexemes += F.lex.v(word)\n",
    "                                #if the lexeme is not the last word of the phrase, we add a '_'.\n",
    "                                subj_lexemes += '_'\n",
    "                            else:\n",
    "                                subj_lexemes += F.lex.v(word)\n",
    "                if subject == True:\n",
    "                    info = [w, where[0], where[1], where[2], F.vt.v(w), F.vs.v(w), F.function.v(phrase), subj_lexemes]\n",
    "                    eat_dict[w] = info\n",
    "                    eat_list.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Martijn\\Documents\\SynVar\\CourseShebanq\\eat_data.csv', \"w\") as csv_file:\n",
    "    \n",
    "    #it is often useful to make a header\n",
    "    header = ['slot', 'book', 'chapter', 'verse', 'tense', 'stem', 'predicate', 'subj_lex']\n",
    "    csv_file.write('{}\\n'.format(','.join(header)))\n",
    "\n",
    "    for case in eat_list:\n",
    "        info_list = eat_dict[case]\n",
    "        line = [str(element) for element in info_list]\n",
    "        csv_file.write('{}\\n'.format(','.join(line)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous examples a so called structured dataset was made, which we saved in a csv file. The dataset is called structured, because it has a fixed format. It consists of a number of columns, and in each column you find the same kind of information for each case in the database.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also be able to make unstructured or semi-structured datasets. An unstructured dataset contains data that are closer to the raw data as we find them in 'nature'. An example is a picture of a Dead Sea Scroll. In a semi-structured dataset the data are structured partly. In our case you could for instance make a text file with the consonantal text of the Hebrew Bible. In the following example, a text file is made in which the biblical text is represented per verse as a sequence of lexemes, separated by strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"lexemes.txt\", \"w\") as lex_file:    \n",
    "    for verse in F.otype.s('verse'):\n",
    "        where = T.sectionFromNode(verse)\n",
    "        #do not forget to make strings of chapter and verse\n",
    "        verse_string = where[0] + ' ' + str(where[1]) + ' ' + str(where[2]) + ' '\n",
    "        words = L.d(verse, 'word')\n",
    "        for word in words:\n",
    "            if word != words[-1]:\n",
    "                verse_string += F.lex.v(word)\n",
    "                verse_string += ' '\n",
    "            else:\n",
    "                verse_string += F.lex.v(word)\n",
    "                #a new verse gets a new line.\n",
    "                verse_string += '\\n'\n",
    "        lex_file.write(verse_string)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous script a file was made that contains the text of the whole MT. If you want to make a separate file for each book, you can do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lexeme_processing(v):\n",
    "    \"\"\" \n",
    "    This function returns a string of lexemes for a verse, which is the input.\n",
    "    It is identical to part of the code you have seen in the previous cell.\n",
    "    \"\"\"\n",
    "    where = T.sectionFromNode(v)\n",
    "    #do not forget to make strings of chapter and verse\n",
    "    verse_string = where[0] + ' ' + str(where[1]) + ' ' + str(where[2]) + ' '\n",
    "    words = L.d(verse, 'word')\n",
    "    for word in words:\n",
    "        if word != words[-1]:\n",
    "            verse_string += F.lex.v(word)\n",
    "            verse_string += ' '\n",
    "        else:\n",
    "            verse_string += F.lex.v(word)\n",
    "            #a new verse gets a new line.\n",
    "            verse_string += '\\n'    \n",
    "    return(verse_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for every book a new file is created.\n",
    "for book in F.otype.s('book'):\n",
    "    book_file = F.book.v(book) + '.txt'\n",
    "    with open(book_file, \"w\") as new_file:\n",
    "        verses = L.d(book, 'verse')\n",
    "        for verse in verses:\n",
    "            #here the function lexeme_processing is called\n",
    "            new_string = lexeme_processing(verse)\n",
    "            new_file.write(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Genesis', 2, 16)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.sectionFromNode(977)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
